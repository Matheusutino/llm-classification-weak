{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matheus/Desktop/Itens./Itens/Projetos/paper-weak-llm\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Folder: Dmoz-Computers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/Desktop/Itens./Itens/Projetos/paper-weak-llm/weak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/matheus/Desktop/Itens./Itens/Projetos/paper-weak-llm/weak/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>folder</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>percent_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deepseek-r1-0528-qwen3-8b.csv</td>\n",
       "      <td>Dmoz-Computers</td>\n",
       "      <td>0.406421</td>\n",
       "      <td>0.515326</td>\n",
       "      <td>0.425667</td>\n",
       "      <td>0.435544</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-nemo.csv</td>\n",
       "      <td>Dmoz-Computers</td>\n",
       "      <td>0.385895</td>\n",
       "      <td>0.457137</td>\n",
       "      <td>0.382632</td>\n",
       "      <td>0.382732</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lfm-7b.csv</td>\n",
       "      <td>Dmoz-Computers</td>\n",
       "      <td>0.327789</td>\n",
       "      <td>0.474677</td>\n",
       "      <td>0.326316</td>\n",
       "      <td>0.333211</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file          folder  accuracy  precision  \\\n",
       "0  deepseek-r1-0528-qwen3-8b.csv  Dmoz-Computers  0.406421   0.515326   \n",
       "2               mistral-nemo.csv  Dmoz-Computers  0.385895   0.457137   \n",
       "1                     lfm-7b.csv  Dmoz-Computers  0.327789   0.474677   \n",
       "\n",
       "     recall  f1_score  percent_none  \n",
       "0  0.425667  0.435544           0.0  \n",
       "2  0.382632  0.382732           0.0  \n",
       "1  0.326316  0.333211           0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics_for_csvs(root_path):\n",
    "    results = []\n",
    "    \n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if 'class' in df.columns and 'predicted_class' in df.columns:\n",
    "                    df['class'] = df['class'].astype(str)\n",
    "                    df['predicted_class'] = df['predicted_class'].astype(str)\n",
    "\n",
    "                    #print(df.isnull().sum())\n",
    "\n",
    "                    # Cálculo de métricas apenas se houver pelo menos uma previsão\n",
    "                    non_null_mask = df['predicted_class'].notna()\n",
    "                    percent_none = 100 * (1 - non_null_mask.mean())  # em %\n",
    "\n",
    "                    acc = accuracy_score(df['class'], df['predicted_class']) if non_null_mask.any() else 0.0\n",
    "                    precision = precision_score(df['class'], df['predicted_class'], average='macro') if non_null_mask.any() else 0.0\n",
    "                    recall = recall_score(df['class'], df['predicted_class'], average='macro') if non_null_mask.any() else 0.0\n",
    "                    f1 = f1_score(df['class'], df['predicted_class'], average='macro') if non_null_mask.any() else 0.0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'file': file,\n",
    "                        'folder': os.path.basename(subdir),\n",
    "                        'accuracy': acc,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1_score': f1,\n",
    "                        'percent_none': round(percent_none, 4)\n",
    "                    })\n",
    "                    print(percent_none)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    \n",
    "    for folder, group in metrics_df.groupby('folder'):\n",
    "        print(f\"Folder: {folder}\")\n",
    "        display(group.sort_values(by='f1_score', ascending=False))\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Exemplo de uso\n",
    "root_directory = \"datasets/llm_predict/zero_shot\"\n",
    "compute_metrics_for_csvs(root_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reportlab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreportlab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpagesizes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m A4\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreportlab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colors\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreportlab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatypus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'reportlab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib import colors\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "def compute_metrics_for_csvs(root_path):\n",
    "    results = []\n",
    "    \n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if 'class' in df.columns and 'predicted_class' in df.columns:\n",
    "                    df = df[df['predicted_class'] != \"other\"]\n",
    "                    df['class'] = df['class'].astype(str)\n",
    "                    df['predicted_class'] = df['predicted_class'].astype(str)\n",
    "                    \n",
    "                    acc = accuracy_score(df['class'], df['predicted_class'])\n",
    "                    precision = precision_score(df['class'], df['predicted_class'], average='macro')\n",
    "                    recall = recall_score(df['class'], df['predicted_class'], average='macro')\n",
    "                    f1 = f1_score(df['class'], df['predicted_class'], average='macro')\n",
    "                    \n",
    "                    results.append({\n",
    "                        'file': file,\n",
    "                        'folder': os.path.basename(subdir),\n",
    "                        'accuracy': f\"{acc:.4f}\",\n",
    "                        'precision': f\"{precision:.4f}\",\n",
    "                        'recall': f\"{recall:.4f}\",\n",
    "                        'f1_score': f\"{f1:.4f}\"\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def generate_pdf(metrics_df, output_path=\"metrics_report.pdf\"):\n",
    "    doc = SimpleDocTemplate(output_path, pagesize=A4)\n",
    "    elements = []\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    for folder, group in metrics_df.groupby('folder'):\n",
    "        # Adiciona título da seção\n",
    "        elements.append(Paragraph(f\"Folder: {folder}\", styles['Heading2']))\n",
    "        elements.append(Spacer(1, 10))\n",
    "        \n",
    "        # Dados da tabela\n",
    "        data = [[\"Arquivo\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]]\n",
    "        for _, row in group.sort_values(by='f1_score', ascending=False).iterrows():\n",
    "            data.append([row['file'], row['accuracy'], row['precision'], row['recall'], row['f1_score']])\n",
    "        \n",
    "        # Criar tabela estilizada\n",
    "        table = Table(data)\n",
    "        style = TableStyle([\n",
    "            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "            ('BOTTOMPADDING', (0, 0), (-1, 0), 8),\n",
    "            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "        ])\n",
    "        table.setStyle(style)\n",
    "        elements.append(table)\n",
    "        elements.append(Spacer(1, 20))  # Espaço entre seções\n",
    "    \n",
    "    # Criar PDF\n",
    "    doc.build(elements)\n",
    "    print(f\"Relatório salvo em {output_path}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "root_directory = \"datasets/llm_predict\"\n",
    "metrics_df = compute_metrics_for_csvs(root_directory)\n",
    "generate_pdf(metrics_df, \"metrics_report.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
